{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation of GNNs and LLMs\n",
    "In this notebook, we train the models on the [MovieLens Dataset](https://movielens.org/) after the Pytorch Geometrics Tutorial on [Link Prediction](https://colab.research.google.com/drive/1xpzn1Nvai1ygd_P5Yambc_oe4VBPK_ZT?usp=sharing#scrollTo=vit8xKCiXAue).\n",
    "\n",
    "First we import all of our dependencies.\n",
    "\n",
    "The **GraphRepresentationGenerator** manages and trains a GNN model. Its most important interfaces include\n",
    "**the constructor**, which defines the GNN architecture and loads the pre-trained GNN model if it is already on the hard disk,\n",
    "**the training method**, which initializes the training on the GNN model and\n",
    "**the get_embedding methods**, which represent the inference interface to the GNN model and return the corresponding embeddings in the dimension defined in the constructor for given user movie node pairs.\n",
    "\n",
    "**The MovieLensLoader** loads and manages the data sets. The most important tasks include **saving and (re)loading and transforming** the data sets.\n",
    "\n",
    "**PromptEncoderOnlyClassifier** and **VanillaEncoderOnlyClassifier** each manage a **prompt (model) LLM** and a **vanilla (model) LLM**. An EncoderOnlyClassifier (ClassifierBase) provides interfaces for training and testing an LLM model.\n",
    "PromptEncoder and VanillaEncoder differ from their DataCollectors. DataCollectors change the behavior of the models during training and testing and allow data points to be created at runtime. With the help of these collators, we **create non-existent edges on the fly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_representation_generator import GraphRepresentationGenerator\n",
    "from dataset_manager import (\n",
    "    MovieLensManager,\n",
    "    PROMPT_KGE_DIMENSION,\n",
    "    INPUT_EMBEDS_REPLACE_KGE_DIMENSION,\n",
    "    ROOT,\n",
    ")\n",
    "from llm_manager import (\n",
    "    PromptBertClassifier,\n",
    "    VanillaBertClassifier,\n",
    "    InputEmbedsReplaceClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define in advance which **Knowledge Graph Embedding Dimension (KGE_DIMENSION)** the GNN encoder has. We want to determine from which output dimension the GNN encoder can produce embeddings that lead to a significant increase in performance *without exceeding the context length of the LLMs*. In the original tutorial, the KGE_DIMENSION was $64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_manager = MovieLensManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MovieLensLoader, which downloads the Movie Lens dataset (https://files.grouplens.org/datasets/movielens/ml-latest-small.zip) and prepares it to be used on GNN and LLM. We also pass the embedding dimensions that we will assume we are training with. First time takes approx. 30 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  source={ node_id=[610] },\n",
       "  target={\n",
       "    node_id=[9742],\n",
       "    x=[9742, 20],\n",
       "  },\n",
       "  (source, edge, target)={ edge_index=[2, 100836] },\n",
       "  (target, rev_edge, source)={ edge_index=[2, 100836] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_manager.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the GNN trainers (possible on Cuda), one for each KGE_DIMENSION.\n",
    "A GNN trainer manages a model and each model consists of an **encoder and classifier** part.\n",
    "\n",
    "**The encoder** is a parameterized *Grap Convolutional Network (GCN)* with a *2-layer GNN computation graph* and a single *ReLU* activation function in between.\n",
    "\n",
    "**The classifier** applies the dot-product between source and destination kges to derive edge-level predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model\n",
      "Device: 'cuda'\n",
      "loading pretrained model\n",
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "graph_representation_generator_prompt = GraphRepresentationGenerator(\n",
    "    kg_manager.data,\n",
    "    kg_manager.gnn_train_data,\n",
    "    kg_manager.gnn_val_data,\n",
    "    kg_manager.gnn_test_data,\n",
    "    kge_dimension=PROMPT_KGE_DIMENSION,\n",
    ")\n",
    "graph_representation_generator_input_embeds_replace = GraphRepresentationGenerator(\n",
    "    kg_manager.data,\n",
    "    kg_manager.gnn_train_data,\n",
    "    kg_manager.gnn_val_data,\n",
    "    kg_manager.gnn_test_data,\n",
    "    hidden_channels=INPUT_EMBEDS_REPLACE_KGE_DIMENSION,\n",
    "    kge_dimension=INPUT_EMBEDS_REPLACE_KGE_DIMENSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train and validate the model on the link prediction task.\n",
    "\n",
    "If the model is already trained, we can skip this part.\n",
    "Training the models can take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prompt Training\")\n",
    "graph_representation_generator_prompt.train_model(\n",
    "    kg_manager.gnn_train_data, EPOCHS, BATCH_SIZE\n",
    ")\n",
    "graph_representation_generator_prompt.validate_model(kg_manager.gnn_test_data)\n",
    "print(\"Attention Training\")\n",
    "graph_representation_generator_input_embeds_replace.train_model(\n",
    "    kg_manager.gnn_train_data, EPOCHS, BATCH_SIZE\n",
    ")\n",
    "graph_representation_generator_input_embeds_replace.validate_model(\n",
    "    kg_manager.gnn_test_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we produce the KGEs for every edge in the dataset. These embeddings can then be used for the LLM on the link-prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings = graph_representation_generator_prompt.get_saved_embeddings(\"prompt\")\n",
    "input_embeds_replace_embeddings = (\n",
    "    graph_representation_generator_input_embeds_replace.get_saved_embeddings(\n",
    "        \"input_embeds_replace\"\n",
    "    )\n",
    ")\n",
    "save = False\n",
    "if prompt_embeddings is None or input_embeds_replace_embeddings is None:\n",
    "    prompt_embeddings = graph_representation_generator_prompt.generate_embeddings(\n",
    "        kg_manager.llm_df\n",
    "    )\n",
    "    input_embeds_replace_embeddings = (\n",
    "        graph_representation_generator_input_embeds_replace.generate_embeddings(\n",
    "            kg_manager.llm_df\n",
    "        )\n",
    "    )\n",
    "    save = True\n",
    "\n",
    "kg_manager.append_prompt_graph_embeddings(prompt_embeddings, save=save)\n",
    "kg_manager.append_input_embeds_replace_graph_embeddings(\n",
    "    input_embeds_replace_embeddings, save=save\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kg_manager.add_false_edges(\n",
    "    1.0,\n",
    "    graph_representation_generator_prompt.get_embedding,\n",
    "    graph_representation_generator_input_embeds_replace.get_embedding,\n",
    "    splits=[\"val\", \"test\"],\n",
    ")\n",
    "df[df[\"split\"] == \"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the vanilla encoder only classifier. This classifier does only use the NLP part of the prompt (no KGE) for predicting if the given link exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VANILLA_ROOT = f\"{ROOT}/llm/vanilla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_bert_classifier = VanillaBertClassifier(\n",
    "    kg_manager.llm_df,\n",
    "    kg_manager.source_df,\n",
    "    kg_manager.target_df,\n",
    "    root_path=VANILLA_ROOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a vanilla llm dataset and tokenize it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vanilla = kg_manager.generate_vanilla_dataset(\n",
    "    vanilla_bert_classifier.tokenize_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model on the produced dataset. This can be skipped, if already trained ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_bert_classifier.train_model_on_data(\n",
    "    dataset_vanilla, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the prompt encoder only classifier. This classifier uses the vanilla prompt and the KGEs for its link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_ROOT = f\"{ROOT}/llm/prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_bert_classifier = PromptBertClassifier(\n",
    "    kg_manager,\n",
    "    graph_representation_generator_prompt.get_embedding,\n",
    "    model_max_length=512,\n",
    "    root_path=PROMPT_ROOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate a prompt dataset, this time the prompts also include the KGEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = kg_manager.generate_prompt_embedding_dataset(\n",
    "    prompt_bert_classifier.tokenize_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also train the model. This can be skipped if already done ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_bert_classifier.train_model_on_data(\n",
    "    dataset_prompt, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EMBEDS_REPLACE_ROOT = f\"{ROOT}/llm/input_embeds_replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds_replace_bert_classifier = InputEmbedsReplaceClassifier(\n",
    "    kg_manager,\n",
    "    graph_representation_generator_input_embeds_replace.get_embedding,\n",
    "    root_path=INPUT_EMBEDS_REPLACE_ROOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_embedding = kg_manager.generate_input_embeds_replace_embedding_dataset(\n",
    "    input_embeds_replace_bert_classifier.tokenizer.sep_token,\n",
    "    input_embeds_replace_bert_classifier.tokenizer.pad_token,\n",
    "    input_embeds_replace_bert_classifier.tokenize_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds_replace_bert_classifier.train_model_on_data(\n",
    "    dataset_embedding, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_manager.add_false_edges(\n",
    "    1.0,\n",
    "    graph_representation_generator_prompt.get_embedding,\n",
    "    graph_representation_generator_input_embeds_replace.get_embedding,\n",
    "    splits=[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_bert_classifier = VanillaBertClassifier(\n",
    "    kg_manager.llm_df,\n",
    "    kg_manager.source_df,\n",
    "    kg_manager.target_df,\n",
    "    root_path=VANILLA_ROOT,\n",
    "    false_ratio=-1,\n",
    ")\n",
    "prompt_bert_classifier = PromptBertClassifier(\n",
    "    kg_manager,\n",
    "    graph_representation_generator_prompt.get_embedding,\n",
    "    root_path=PROMPT_ROOT,\n",
    "    model_max_length=512,\n",
    "    false_ratio=-1,\n",
    ")\n",
    "input_embeds_replace_bert_classifier = InputEmbedsReplaceClassifier(\n",
    "    kg_manager,\n",
    "    graph_representation_generator_input_embeds_replace.get_embedding,\n",
    "    root_path=INPUT_EMBEDS_REPLACE_ROOT,\n",
    "    false_ratio=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vanilla = kg_manager.generate_vanilla_dataset(\n",
    "    vanilla_bert_classifier.tokenize_function\n",
    ")\n",
    "dataset_prompt = kg_manager.generate_prompt_embedding_dataset(\n",
    "    prompt_bert_classifier.tokenize_function,\n",
    ")\n",
    "dataset_input_embeds_replace = (\n",
    "    kg_manager.generate_input_embeds_replace_embedding_dataset(\n",
    "        input_embeds_replace_bert_classifier.tokenizer.sep_token,\n",
    "        input_embeds_replace_bert_classifier.tokenizer.pad_token,\n",
    "        input_embeds_replace_bert_classifier.tokenize_function,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/llm/vanilla/attentions.npy ./data/llm/vanilla/hidden_states.npy ./data/llm/vanilla/tokens.csv\n",
      "./data/llm/prompt/attentions.npy ./data/llm/prompt/hidden_states.npy ./data/llm/prompt/tokens.csv\n",
      "./data/llm/input_embeds_replace/attentions.npy ./data/llm/input_embeds_replace/hidden_states.npy ./data/llm/input_embeds_replace/tokens.csv\n"
     ]
    }
   ],
   "source": [
    "vanilla_df = vanilla_bert_classifier.forward_dataset_and_save_outputs(\n",
    "    dataset_vanilla,\n",
    "    kg_manager.get_vanilla_tokens_as_df,\n",
    "    epochs=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    force_recompute=False,\n",
    ")\n",
    "prompt_df = prompt_bert_classifier.forward_dataset_and_save_outputs(\n",
    "    dataset_prompt,\n",
    "    kg_manager.get_prompt_tokens_as_df,\n",
    "    epochs=1,\n",
    "    force_recompute=False,\n",
    ")\n",
    "input_embeds_replace_df = (\n",
    "    input_embeds_replace_bert_classifier.forward_dataset_and_save_outputs(\n",
    "        dataset_input_embeds_replace,\n",
    "        kg_manager.get_vanilla_tokens_as_df,\n",
    "        epochs=1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        force_recompute=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>labels</th>\n",
       "      <th>split</th>\n",
       "      <th>hidden_states</th>\n",
       "      <th>attentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>heat ( 1995 )</td>\n",
       "      <td>['action ', 'crime ', 'thriller']</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.16464299, 0.058689594], [0.1322025, 0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>bottle rocket ( 1996 )</td>\n",
       "      <td>['adventure ', 'comedy ', 'crime ', 'romance']</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.087070346, 0.08129981], [0.057338703, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>braveheart ( 1995 )</td>\n",
       "      <td>['action ', 'drama ', 'war']</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.18892497, 0.073472254], [0.15924668, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>billy madison ( 1995 )</td>\n",
       "      <td>['comedy']</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.13144213, 0.07449591], [0.10240837, 0.041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>star wars : episode iv - a new hope ( 1977 )</td>\n",
       "      <td>['action ', 'adventure ', 'sci - fi']</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.1672216, 0.03307326], [0.13944641, 0.0127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125032</th>\n",
       "      <td>79</td>\n",
       "      <td>8846</td>\n",
       "      <td>lovesick ( 2014 )</td>\n",
       "      <td>['comedy ', 'romance']</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.084072135, 0.103651546], [0.031487536, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125033</th>\n",
       "      <td>300</td>\n",
       "      <td>2595</td>\n",
       "      <td>dersu uzala ( 1975 )</td>\n",
       "      <td>['adventure ', 'drama']</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.06681271, 0.023924373], [0.022649713, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125034</th>\n",
       "      <td>321</td>\n",
       "      <td>59</td>\n",
       "      <td>lawnmower man 2 : beyond cyberspace ( 1996 )</td>\n",
       "      <td>['action ', 'sci - fi ', 'thriller']</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.11401038, 0.042020373], [0.033344958, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125035</th>\n",
       "      <td>388</td>\n",
       "      <td>2639</td>\n",
       "      <td>all the vermeers in new york ( 1990 )</td>\n",
       "      <td>['comedy ', 'drama ', 'romance']</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.049182758, 0.040349208], [0.017131926, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125036</th>\n",
       "      <td>562</td>\n",
       "      <td>5853</td>\n",
       "      <td>fever pitch ( 2005 )</td>\n",
       "      <td>['comedy ', 'romance']</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>[[[0.9417381, 0.11100921, -8.244461, 0.0768013...</td>\n",
       "      <td>[[[0.082226604, 0.07485455], [0.024612425, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125037 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_id  target_id                                         title  \\\n",
       "0               0          5                                 heat ( 1995 )   \n",
       "1               0         89                        bottle rocket ( 1996 )   \n",
       "2               0         97                           braveheart ( 1995 )   \n",
       "3               0        184                        billy madison ( 1995 )   \n",
       "4               0        224  star wars : episode iv - a new hope ( 1977 )   \n",
       "...           ...        ...                                           ...   \n",
       "125032         79       8846                             lovesick ( 2014 )   \n",
       "125033        300       2595                          dersu uzala ( 1975 )   \n",
       "125034        321         59  lawnmower man 2 : beyond cyberspace ( 1996 )   \n",
       "125035        388       2639         all the vermeers in new york ( 1990 )   \n",
       "125036        562       5853                          fever pitch ( 2005 )   \n",
       "\n",
       "                                                genres  labels  split  \\\n",
       "0                    ['action ', 'crime ', 'thriller']       1  train   \n",
       "1       ['adventure ', 'comedy ', 'crime ', 'romance']       1  train   \n",
       "2                         ['action ', 'drama ', 'war']       1  train   \n",
       "3                                           ['comedy']       1  train   \n",
       "4                ['action ', 'adventure ', 'sci - fi']       1  train   \n",
       "...                                                ...     ...    ...   \n",
       "125032                          ['comedy ', 'romance']       0    val   \n",
       "125033                         ['adventure ', 'drama']       0    val   \n",
       "125034            ['action ', 'sci - fi ', 'thriller']       0    val   \n",
       "125035                ['comedy ', 'drama ', 'romance']       0    val   \n",
       "125036                          ['comedy ', 'romance']       0    val   \n",
       "\n",
       "                                            hidden_states  \\\n",
       "0       [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "1       [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "2       [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "3       [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "4       [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "...                                                   ...   \n",
       "125032  [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "125033  [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "125034  [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "125035  [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "125036  [[[0.9417381, 0.11100921, -8.244461, 0.0768013...   \n",
       "\n",
       "                                               attentions  \n",
       "0       [[[0.16464299, 0.058689594], [0.1322025, 0.042...  \n",
       "1       [[[0.087070346, 0.08129981], [0.057338703, 0.0...  \n",
       "2       [[[0.18892497, 0.073472254], [0.15924668, 0.03...  \n",
       "3       [[[0.13144213, 0.07449591], [0.10240837, 0.041...  \n",
       "4       [[[0.1672216, 0.03307326], [0.13944641, 0.0127...  \n",
       "...                                                   ...  \n",
       "125032  [[[0.084072135, 0.103651546], [0.031487536, 0....  \n",
       "125033  [[[0.06681271, 0.023924373], [0.022649713, 0.0...  \n",
       "125034  [[[0.11401038, 0.042020373], [0.033344958, 0.0...  \n",
       "125035  [[[0.049182758, 0.040349208], [0.017131926, 0....  \n",
       "125036  [[[0.082226604, 0.07485455], [0.024612425, 0.0...  \n",
       "\n",
       "[125037 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds_replace_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = kg_manager.generate_huggingface_dataset(\n",
    "    [vanilla_df, prompt_df, input_embeds_replace_df],\n",
    "    [\"vanilla\", \"prompt\", \"input_embeds_replace\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddab380cc764d54bca93b24734a9d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/56469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b808b2e141a42968ed16c4d2511864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/34284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28abe357a11b4ad1a77db6aab200b64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/34284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./data/dataset.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d190519f7e764c8e9f8a55754d2a8ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b431a65fd52246f5ace2b5e4240035ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8718e67ed8428a9bdb8344e89a0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8471978139444cea9918215e9261c0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9656356942244bedab2250d57981420f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5154491527da493386f2af3f3954e1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d9dd4f006a4b14ac0f028410ba73c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bbaebcb15c441a863c6492b6d809ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4aaf3344044258a3aa50e0a6c60ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd1cbfb6adb4b4a95b1c2fc560bd7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b823db884a42569ade2350f3bc2846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6105c54b0e73460e9cdc34f9901be355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657134f58f654d5396b7fafe8f37ef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ed9845f93f4901a6252badf59eae9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8200d0fa19074f2cb214ad035ab69a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3a8b3c044945f6a264cba4b157260f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4ad6883c9e42f69b572570102737de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505c719da59f420f80206d5715461ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d67d0d8f2dd4f8e8c7c528374490348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MARS\\.cache\\huggingface\\hub\\datasets--AhmadPython--MovieLens_KGE. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AhmadPython/MovieLens_KGE/commit/60a2bc61bdf043539b0e3091c46ba6bd0fa0c931', commit_message='Upload dataset', commit_description='', oid='60a2bc61bdf043539b0e3091c46ba6bd0fa0c931', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"AhmadPython/MovieLens_KGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_id', 'target_id', 'id_x', 'id_y', 'prompt_feature_title', 'prompt_feature_genres', 'labels', 'split', 'prompt', 'gnn_feature_(no genres listed)', 'gnn_feature_Action', 'gnn_feature_Adventure', 'gnn_feature_Animation', 'gnn_feature_Children', 'gnn_feature_Comedy', 'gnn_feature_Crime', 'gnn_feature_Documentary', 'gnn_feature_Drama', 'gnn_feature_Fantasy', 'gnn_feature_Film-Noir', 'gnn_feature_Horror', 'gnn_feature_IMAX', 'gnn_feature_Musical', 'gnn_feature_Mystery', 'gnn_feature_Romance', 'gnn_feature_Sci-Fi', 'gnn_feature_Thriller', 'gnn_feature_War', 'gnn_feature_Western', 'vanilla_attentions', 'vanilla_hidden_states', 'vanilla_attentions_original_shape', 'vanilla_hidden_states_original_shape', 'prompt_attentions', 'prompt_hidden_states', 'prompt_attentions_original_shape', 'prompt_hidden_states_original_shape', 'input_embeds_replace_attentions', 'input_embeds_replace_hidden_states', 'input_embeds_replace_attentions_original_shape', 'input_embeds_replace_hidden_states_original_shape'],\n",
       "        num_rows: 56469\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source_id', 'target_id', 'id_x', 'id_y', 'prompt_feature_title', 'prompt_feature_genres', 'labels', 'split', 'prompt', 'gnn_feature_(no genres listed)', 'gnn_feature_Action', 'gnn_feature_Adventure', 'gnn_feature_Animation', 'gnn_feature_Children', 'gnn_feature_Comedy', 'gnn_feature_Crime', 'gnn_feature_Documentary', 'gnn_feature_Drama', 'gnn_feature_Fantasy', 'gnn_feature_Film-Noir', 'gnn_feature_Horror', 'gnn_feature_IMAX', 'gnn_feature_Musical', 'gnn_feature_Mystery', 'gnn_feature_Romance', 'gnn_feature_Sci-Fi', 'gnn_feature_Thriller', 'gnn_feature_War', 'gnn_feature_Western', 'vanilla_attentions', 'vanilla_hidden_states', 'vanilla_attentions_original_shape', 'vanilla_hidden_states_original_shape', 'prompt_attentions', 'prompt_hidden_states', 'prompt_attentions_original_shape', 'prompt_hidden_states_original_shape', 'input_embeds_replace_attentions', 'input_embeds_replace_hidden_states', 'input_embeds_replace_attentions_original_shape', 'input_embeds_replace_hidden_states_original_shape'],\n",
       "        num_rows: 34284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source_id', 'target_id', 'id_x', 'id_y', 'prompt_feature_title', 'prompt_feature_genres', 'labels', 'split', 'prompt', 'gnn_feature_(no genres listed)', 'gnn_feature_Action', 'gnn_feature_Adventure', 'gnn_feature_Animation', 'gnn_feature_Children', 'gnn_feature_Comedy', 'gnn_feature_Crime', 'gnn_feature_Documentary', 'gnn_feature_Drama', 'gnn_feature_Fantasy', 'gnn_feature_Film-Noir', 'gnn_feature_Horror', 'gnn_feature_IMAX', 'gnn_feature_Musical', 'gnn_feature_Mystery', 'gnn_feature_Romance', 'gnn_feature_Sci-Fi', 'gnn_feature_Thriller', 'gnn_feature_War', 'gnn_feature_Western', 'vanilla_attentions', 'vanilla_hidden_states', 'vanilla_attentions_original_shape', 'vanilla_hidden_states_original_shape', 'prompt_attentions', 'prompt_hidden_states', 'prompt_attentions_original_shape', 'prompt_hidden_states_original_shape', 'input_embeds_replace_attentions', 'input_embeds_replace_hidden_states', 'input_embeds_replace_attentions_original_shape', 'input_embeds_replace_hidden_states_original_shape'],\n",
       "        num_rows: 34284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grundprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
