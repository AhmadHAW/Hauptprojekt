{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training and Evaluation of GNNs and LLMs\n",
                "In this notebook, we train the models on the [MovieLens Dataset](https://movielens.org/) after the Pytorch Geometrics Tutorial on [Link Prediction](https://colab.research.google.com/drive/1xpzn1Nvai1ygd_P5Yambc_oe4VBPK_ZT?usp=sharing#scrollTo=vit8xKCiXAue).\n",
                "\n",
                "First we import all of our dependencies.\n",
                "\n",
                "The **GraphRepresentationGenerator** manages and trains a GNN model. Its most important interfaces include\n",
                "**the constructor**, which defines the GNN architecture and loads the pre-trained GNN model if it is already on the hard disk,\n",
                "**the training method**, which initializes the training on the GNN model and\n",
                "**the get_embedding methods**, which represent the inference interface to the GNN model and return the corresponding embeddings in the dimension defined in the constructor for given user movie node pairs.\n",
                "\n",
                "**The MovieLensLoader** loads and manages the data sets. The most important tasks include **saving and (re)loading and transforming** the data sets.\n",
                "\n",
                "**PromptEncoderOnlyClassifier** and **VanillaEncoderOnlyClassifier** each manage a **prompt (model) LLM** and a **vanilla (model) LLM**. An EncoderOnlyClassifier (ClassifierBase) provides interfaces for training and testing an LLM model.\n",
                "PromptEncoder and VanillaEncoder differ from their DataCollectors. DataCollectors change the behavior of the models during training and testing and allow data points to be created at runtime. With the help of these collators, we **create non-existent edges on the fly**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from graph_representation_generator.graph_representation_generator import (\n",
                "    GraphRepresentationGenerator,\n",
                ")\n",
                "from dataset_manager.movie_lens_manager import MovieLensManager\n",
                "from dataset_manager.kg_manager import ROOT\n",
                "from llm_manager.graph_prompter_hf.classifier import GraphPrompterHF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
                "EPOCHS = 1\n",
                "BATCH_SIZE_LLM = 256"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We define in advance which **Knowledge Graph Embedding Dimension (KGE_DIMENSION)** the GNN encoder has. We want to determine from which output dimension the GNN encoder can produce embeddings that lead to a significant increase in performance *without exceeding the context length of the LLMs*. In the original tutorial, the KGE_DIMENSION was $64$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "kg_manager = MovieLensManager()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "llm_df = kg_manager.llm_df.merge(kg_manager.target_df[[\"id\", \"prompt_feature_title\", \"prompt_feature_genres\"]].rename(columns={\"id\": \"target_id\"}), on = \"target_id\")\n",
                "llm_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First we load the MovieLensLoader, which downloads the Movie Lens dataset (https://files.grouplens.org/datasets/movielens/ml-32m.zip) and prepares it to be used on GNN and LLM. We also pass the embedding dimensions that we will assume we are training with. First time takes approx. 30 sec."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "HeteroData(\n",
                            "  source={ node_id=[200948] },\n",
                            "  target={\n",
                            "    node_id=[87585],\n",
                            "    x=[87585, 20],\n",
                            "  },\n",
                            "  (source, edge, target)={ edge_index=[2, 32000204] },\n",
                            "  (target, rev_edge, source)={ edge_index=[2, 32000204] }\n",
                            ")"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "kg_manager.data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we initialize the GNN trainers (possible on Cuda), one for each KGE_DIMENSION.\n",
                "A GNN trainer manages a model and each model consists of an **encoder and classifier** part.\n",
                "\n",
                "**The encoder** is a parameterized *Grap Convolutional Network (GCN)* with a *2-layer GNN computation graph* and a single *ReLU* activation function in between.\n",
                "\n",
                "**The classifier** applies the dot-product between source and destination kges to derive edge-level predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "loading pretrained model\n",
                        "Device: 'cpu'\n"
                    ]
                }
            ],
            "source": [
                "graph_representation_generator_graph_prompter_hf = GraphRepresentationGenerator(\n",
                "    kg_manager.data,\n",
                "    kg_manager.gnn_train_data,\n",
                "    kg_manager.gnn_val_data,\n",
                "    kg_manager.gnn_test_data,\n",
                "    hidden_channels=128,\n",
                "    kge_dimension=128,\n",
                "    force_recompute=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next we produce the KGEs for every edge in the dataset. These embeddings can then be used for the LLM on the link-prediction task."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next we initialize the vanilla encoder only classifier. This classifier does only use the NLP part of the prompt (no KGE) for predicting if the given link exists."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of GraphPrompterHFBertForSequenceClassification were not initialized from the model checkpoint at ./data/llm/vanilla/training/best and are newly initialized because the shapes did not match:\n",
                        "- bert.embeddings.token_type_embeddings.weight: found shape torch.Size([3, 128]) in the checkpoint and torch.Size([5, 128]) in the model instantiated\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "5\n",
                        "5\n",
                        "5\n",
                        "device cuda\n"
                    ]
                }
            ],
            "source": [
                "graph_prompter_hf_bert_classifier = GraphPrompterHF(\n",
                "    kge_manager=kg_manager,\n",
                "    get_embeddings_cb=graph_representation_generator_graph_prompter_hf.get_embeddings,\n",
                "    model_name=MODEL_NAME,\n",
                "    vanilla_model_path=f\"{ROOT}/llm/vanilla/training/best\",\n",
                "    # gnn_parameters=list(\n",
                "    #    graph_representation_generator_graph_prompter_hf.model.parameters()\n",
                "    # ),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6b6e31e44bfc4b10ae95a33eba42cdd2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/17920114 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "dataset_embedding = kg_manager.generate_graph_prompter_hf_embedding_dataset(\n",
                "    graph_prompter_hf_bert_classifier.tokenizer.sep_token,\n",
                "    graph_prompter_hf_bert_classifier.tokenizer.pad_token,\n",
                "    graph_prompter_hf_bert_classifier.tokenize_function,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "import torch\n",
                "\n",
                "gnn_model = graph_representation_generator_graph_prompter_hf.model\n",
                "trainer = graph_prompter_hf_bert_classifier._get_trainer(\n",
                "    dataset_embedding, epochs=EPOCHS, batch_size=BATCH_SIZE_LLM\n",
                ")\n",
                "optimizer = torch.optim.SGD(\n",
                "    list(trainer.model.parameters()) + list(gnn_model.parameters()),\n",
                "    lr=0.001,\n",
                "    momentum=0.9,\n",
                ")\n",
                "\n",
                "\n",
                "def inspect_gradients(trainer, batch):\n",
                "    optimizer.zero_grad()  # Clear previous gradients\n",
                "    trainer.model.eval()  # Set model to eval mode (no dropout, etc.)\n",
                "    outputs = trainer.model(**batch)\n",
                "    loss = outputs.loss\n",
                "    loss.backward()  # Run backward to compute gradients\n",
                "    optimizer.step()\n",
                "\n",
                "\n",
                "# Example of grabbing a batch and inspecting gradients\n",
                "for i in range(1):\n",
                "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
                "\n",
                "    inspect_gradients(trainer, sample_batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "target_lin.weight is not same\n",
                        "target_lin.bias is not same\n",
                        "source_emb.weight is not same\n",
                        "target_emb.weight is not same\n",
                        "gnn.conv1.source__edge__target.lin_l.weight is not same\n",
                        "gnn.conv1.source__edge__target.lin_l.bias is not same\n",
                        "gnn.conv1.source__edge__target.lin_r.weight is not same\n",
                        "gnn.conv1.target__rev_edge__source.lin_l.weight is not same\n",
                        "gnn.conv1.target__rev_edge__source.lin_l.bias is not same\n",
                        "gnn.conv1.target__rev_edge__source.lin_r.weight is not same\n",
                        "gnn.conv2.source__edge__target.lin_l.weight is not same\n",
                        "gnn.conv2.source__edge__target.lin_l.bias is not same\n",
                        "gnn.conv2.source__edge__target.lin_r.weight is not same\n",
                        "gnn.conv2.target__rev_edge__source.lin_l.weight is not same\n",
                        "gnn.conv2.target__rev_edge__source.lin_l.bias is not same\n",
                        "gnn.conv2.target__rev_edge__source.lin_r.weight is not same\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "tens_1 = graph_representation_generator_graph_prompter_hf.model.to(\n",
                "    device=\"cpu\"\n",
                ").state_dict()\n",
                "\n",
                "tens_2 = torch.load(\"./data/gnn/backup/model_128.pth\")\n",
                "for key in tens_1.keys():\n",
                "    if isinstance(tens_1[key], torch.Tensor):\n",
                "        if (tens_1[key] == tens_2[key]).all():\n",
                "            print(key, \"is same\")\n",
                "        else:\n",
                "            print(key, \"is not same\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f2ad166d909d4ae38d5c2661a0a87f92",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/100001 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
                        "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
                        "  File \"<frozen runpy>\", line 88, in _run_code\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
                        "    app.launch_new_instance()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
                        "    app.start()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
                        "    self.io_loop.start()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
                        "    self.asyncio_loop.run_forever()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
                        "    self._run_once()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
                        "    handle._run()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\asyncio\\events.py\", line 88, in _run\n",
                        "    self._context.run(self._callback, *self._args)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
                        "    await self.process_one()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
                        "    await dispatch(*args)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
                        "    await result\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
                        "    await super().execute_request(stream, ident, parent)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
                        "    reply_content = await reply_content\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
                        "    res = shell.run_cell(\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
                        "    return super().run_cell(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
                        "    result = self._run_cell(\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
                        "    result = runner(coro)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
                        "    coro.send(None)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
                        "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
                        "    if await self.run_code(code, result, async_=asy):\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
                        "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
                        "  File \"C:\\Users\\MARS\\AppData\\Local\\Temp\\ipykernel_15480\\3413638375.py\", line 1, in <module>\n",
                        "    graph_prompter_hf_bert_classifier.train_model_on_data(\n",
                        "  File \"c:\\Users\\MARS\\Ahmad\\Hauptprojekt\\llm_manager.py\", line 982, in train_model_on_data\n",
                        "    trainer.train()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\transformers\\trainer.py\", line 1932, in train\n",
                        "    return inner_training_loop(\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\transformers\\trainer.py\", line 2230, in _inner_training_loop\n",
                        "    for step, inputs in enumerate(epoch_iterator):\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\accelerate\\data_loader.py\", line 464, in __iter__\n",
                        "    next_batch = next(dataloader_iter)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\n",
                        "    data = self._next_data()\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 675, in _next_data\n",
                        "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
                        "    return self.collate_fn(data)\n",
                        "  File \"c:\\Users\\MARS\\Ahmad\\Hauptprojekt\\llm_manager.py\", line 178, in __call__\n",
                        "    return self._convert_features_into_batches(features)\n",
                        "  File \"c:\\Users\\MARS\\Ahmad\\Hauptprojekt\\llm_manager.py\", line 263, in _convert_features_into_batches\n",
                        "    source_embeddings, target_embeddings = self.get_embeddings_cb(\n",
                        "  File \"c:\\Users\\MARS\\Ahmad\\Hauptprojekt\\graph_representation_generator.py\", line 284, in get_embeddings\n",
                        "    embeddings = self.model.forward_without_classifier(sampled_data)\n",
                        "  File \"c:\\Users\\MARS\\Ahmad\\Hauptprojekt\\graph_representation_generator.py\", line 109, in forward_without_classifier\n",
                        "    return self.gnn(x_dict, data.edge_index_dict)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\fx\\graph_module.py\", line 737, in call_wrapped\n",
                        "    return self._wrapped_call(self, *args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\fx\\graph_module.py\", line 304, in __call__\n",
                        "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"<eval_with_key>.1\", line 16, in forward\n",
                        "    conv2__source = self.conv2.target__rev_edge__source((relu__target, relu__source), edge_index__target__rev_edge__source);  relu__target = relu__source = edge_index__target__rev_edge__source = None\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\", line 139, in forward\n",
                        "    out = out + self.lin_r(x_r)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py\", line 147, in forward\n",
                        "    return F.linear(x, self.weight, self.bias)\n",
                        " (Triggered internally at ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:116.)\n",
                        "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 128]], which is output 0 of AsStridedBackward0, is at version 6; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgraph_prompter_hf_bert_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model_on_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE_LLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\Ahmad\\Hauptprojekt\\llm_manager.py:982\u001b[0m, in \u001b[0;36mClassifierBase.train_model_on_data\u001b[1;34m(self, dataset, epochs, batch_size)\u001b[0m\n\u001b[0;32m    979\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_trainer(dataset, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model_path)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    985\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2274\u001b[0m ):\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\transformers\\trainer.py:3324\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3322\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\accelerate\\accelerator.py:2151\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2151\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 128]], which is output 0 of AsStridedBackward0, is at version 6; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
                    ]
                }
            ],
            "source": [
                "graph_prompter_hf_bert_classifier.train_model_on_data(\n",
                "    dataset_embedding,\n",
                "    epochs=EPOCHS,\n",
                "    batch_size=BATCH_SIZE_LLM,\n",
                ")\n",
                "graph_representation_generator_graph_prompter_hf.save_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "hauptprojekt",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
