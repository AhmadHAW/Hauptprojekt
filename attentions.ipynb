{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import (\n",
    "    VANILLA_ATTENTIONS_PATH,\n",
    "    PROMPT_ATTENTIONS_PATH,\n",
    "    EMBEDDING_ATTENTIONS_PATH,\n",
    "    VANILLA_TOKENS_PATH,\n",
    "    PROMPT_TOKENS_PATH,\n",
    "    EMBEDDING_TOKENS_PATH,\n",
    ")\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attentions(\n",
    "    path_to_attention_npy: str | Path,\n",
    "    split: Optional[str] = None,\n",
    "    path_to_df: Optional[str | Path] = None,\n",
    ") -> torch.Tensor:\n",
    "    attentions = np.load(path_to_attention_npy)\n",
    "    if split:\n",
    "        assert path_to_df\n",
    "        df = pd.read_csv(path_to_df)\n",
    "        df[\"attentions\"] = list(attentions)\n",
    "        attentions = df[df[\"split\"] == split][\"attentions\"].values\n",
    "        attentions = np.stack(attentions)  # type: ignore\n",
    "        return torch.from_numpy(attentions)\n",
    "    else:\n",
    "        return torch.from_numpy(attentions)\n",
    "\n",
    "\n",
    "def plot_attention_graph(\n",
    "    attentions: torch.Tensor,\n",
    "    token_labels: List[str],\n",
    "    title: str,\n",
    "    weight_coef: int | float = 5,\n",
    "    fig_dpi: int = 100,\n",
    "    fig_size: Tuple[int, int] = (8, 8),\n",
    "    save_path: Optional[str | Path] = None,\n",
    ") -> None:\n",
    "    assert weight_coef > 0\n",
    "    assert fig_dpi > 0\n",
    "    assert fig_size[0] > 0\n",
    "    assert fig_size[1] > 0\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.rcParams[\"figure.dpi\"] = fig_dpi  # 200 e.g. is really fine, but slower\n",
    "    plt.figure(figsize=fig_size, dpi=fig_dpi)\n",
    "    # Create an undirected graph\n",
    "    G = nx.Graph()\n",
    "    labels = {}\n",
    "    attentions = torch.mean(attentions, dim=0).permute((2, 0, 1))\n",
    "    for layer, attentions_ in enumerate(attentions):\n",
    "        for from_, inner in enumerate(attentions_):\n",
    "            from_name = f\"{token_labels[from_]}_{layer}\"\n",
    "            if layer == 0:\n",
    "                labels[from_name] = token_labels[from_]\n",
    "            G.add_node(from_name, name=from_name, layer=layer)\n",
    "            for to_, weight in enumerate(inner):\n",
    "                to_name = f\"{token_labels[to_]}_{layer+1}\"\n",
    "                G.add_node(to_name, name=to_name, layer=layer + 1)\n",
    "                G.add_edge(from_name, to_name, weight=weight)\n",
    "\n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
    "    semantic_datapoints = token_labels.copy()\n",
    "    semantic_datapoints.reverse()\n",
    "    for node, (x, y) in pos.items():\n",
    "        y = semantic_datapoints.index(node.split(\"_\")[0])\n",
    "        layer = G.nodes[node][\"layer\"]\n",
    "        pos[node] = (layer, y)  # type: ignore # Fixing the x-coordinate to be the layer and y-coordinate can be customized\n",
    "\n",
    "    nx.draw(G, pos=pos, with_labels=False)\n",
    "\n",
    "    edge_weights = nx.get_edge_attributes(G, \"weight\")\n",
    "\n",
    "    # Create a list of edge thicknesses based on weights\n",
    "    # Normalize the weights to get thickness values\n",
    "    max_weight = max(edge_weights.values())\n",
    "    edge_thickness = [\n",
    "        edge_weights[edge] / max_weight * weight_coef for edge in G.edges()\n",
    "    ]  # Draw edges with varying thicknesses\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=G.edges(), width=edge_thickness)  # type: ignore\n",
    "    shift_to_left = 0.4\n",
    "    label_pos = {node: (x - shift_to_left, y) for node, (x, y) in pos.items()}\n",
    "    nx.draw_networkx_labels(G, label_pos, labels)\n",
    "    # Get the current axis limits\n",
    "    x_min, x_max = plt.xlim()\n",
    "\n",
    "    # Adjust x-axis limits to add more whitespace on the left\n",
    "    plt.xlim(x_min - (shift_to_left + 0.1), x_max)  # Increase the left limit by 0.1\n",
    "    plt.title(title)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_vanilla_attentions = VANILLA_ATTENTIONS_PATH  # replace with the respective path\n",
    "path_to_vanilla_df = VANILLA_TOKENS_PATH  # only nessecary if we want to split the attentions in val, test or train. DF has same length as attentions with column \"split\"\n",
    "vanilla_attentions = load_attentions(\n",
    "    path_to_vanilla_attentions, path_to_df=path_to_vanilla_df, split=\"val\"\n",
    ")\n",
    "vanilla_attentions.shape  # shape [dataset_size, from positions, to positions, layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_token_labels = [\"cls\", \"user\", \"sep1\", \"title\", \"sep2\", \"genres\", \"sep3\"]\n",
    "plot_attention_graph(\n",
    "    vanilla_attentions,\n",
    "    vanilla_token_labels,\n",
    "    \"Vanilla Model Attentions Plot\",\n",
    "    weight_coef=10,\n",
    "    save_path=\"./images/Vanilla_Model_Attentions.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_prompt_attentions = PROMPT_ATTENTIONS_PATH  # replace with the respective path\n",
    "path_to_embedding_attentions = (\n",
    "    EMBEDDING_ATTENTIONS_PATH  # replace with the respective path\n",
    ")\n",
    "path_to_prompt_df = PROMPT_TOKENS_PATH\n",
    "path_to_embedding_df = EMBEDDING_TOKENS_PATH\n",
    "prompt_attentions = load_attentions(\n",
    "    path_to_prompt_attentions, path_to_df=path_to_prompt_df, split=\"val\"\n",
    ")\n",
    "embedding_attentions = load_attentions(\n",
    "    path_to_embedding_attentions, path_to_df=path_to_embedding_df, split=\"val\"\n",
    ")\n",
    "prompt_attentions.shape, embedding_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_token_labels = [\n",
    "    \"cls\",\n",
    "    \"user\",\n",
    "    \"sep1\",\n",
    "    \"title\",\n",
    "    \"sep2\",\n",
    "    \"genres\",\n",
    "    \"sep3\",\n",
    "    \"user embedding\",\n",
    "    \"sep4\",\n",
    "    \"movie embedding\",\n",
    "    \"sep5\",\n",
    "]\n",
    "plot_attention_graph(\n",
    "    prompt_attentions,\n",
    "    embedding_token_labels,\n",
    "    \"Prompt Model Attentions Plot\",\n",
    "    weight_coef=10,\n",
    "    save_path=\"./images/Prompt_Model_Attentions.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_graph(\n",
    "    embedding_attentions,\n",
    "    embedding_token_labels,\n",
    "    \"Attention Model Attentions Plot\",\n",
    "    weight_coef=10,\n",
    "    save_path=\"./images/Attention_model_Attentions.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grundprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
