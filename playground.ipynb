{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn import GNNTrainer\n",
    "from movie_lens_loader import MovieLensLoader\n",
    "from llm import PromptEncoderOnlyClassifier, VanillaEncoderOnlyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MovieLensLoader, which downloads the Movie Lens dataset (https://files.grouplens.org/datasets/movielens/ml-latest-small.zip) and prepares it to be used on GNN and LLM (approximatly 30 secs first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_lens_loader = MovieLensLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the GNNTrainer, which expects the complete dataset to read the dataset schema. The GNNTrainer can later be used to train in link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_trainer = GNNTrainer(movie_lens_loader.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train and validate the model on the link prediction task. If the model is already trained, we can skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_trainer.train_model(movie_lens_loader.gnn_train_data, 10)\n",
    "#gnn_trainer.validate_model(movie_lens_loader.gnn_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __get_embedding(self, row, movie_lens_loader: MovieLensLoader):\n",
    "            split = row[\"split\"]\n",
    "            data = movie_lens_loader.gnn_train_data if split == \"train\" else movie_lens_loader.gnn_val_data if split == \"val\" else movie_lens_loader.gnn_test_data if split == \"test\" else movie_lens_loader.gnn_train_data\n",
    "            user_id = row[\"mappedUserId\"]\n",
    "            movie_id = row[\"mappedMovieId\"]\n",
    "            user_embedding, movie_embedding, _, _ = self.get_embedding(data, user_id, movie_id)\n",
    "            row[\"user_embedding\"] = user_embedding.detach().tolist()\n",
    "            row[\"movie_embedding\"] = movie_embedding.detach().tolist()\n",
    "            return row\n",
    "        \n",
    "#produce the embeddings for all edges\n",
    "df = movie_lens_loader.llm_df.apply(lambda row: __get_embedding(gnn_trainer, row, movie_lens_loader), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress the embeddings of all user embeddings to two dimensions\n",
    "df[f\"pca_2_user_embedding\"] = \"\"\n",
    "for split in [\"train\", \"test\", \"val\", \"rest\"]:\n",
    "    print(split)\n",
    "    condition = df['split'] == split\n",
    "    user_embeddings = list(df[condition][\"user_embedding\"].values)\n",
    "    if split == \"train\":\n",
    "        if not gnn_trainer.user_pca_train or gnn_trainer.force_recompute:\n",
    "            gnn_trainer.user_pca_train = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "        pca = gnn_trainer.user_pca_train\n",
    "    if split == \"val\":\n",
    "        if not gnn_trainer.user_pca_val or gnn_trainer.force_recompute:\n",
    "            gnn_trainer.user_pca_val = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "        pca = gnn_trainer.user_pca_val\n",
    "    if split == \"test\":\n",
    "        if not gnn_trainer.user_pca_test or gnn_trainer.force_recompute:\n",
    "            gnn_trainer.user_pca_test = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "        pca = gnn_trainer.user_pca_test\n",
    "    if split == \"rest\":\n",
    "        if not gnn_trainer.user_pca_rest or gnn_trainer.force_recompute:\n",
    "            gnn_trainer.user_pca_rest = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "        pca = gnn_trainer.user_pca_rest\n",
    "    print(len(user_embeddings), len(user_embeddings[0]))\n",
    "    print(pca.fit_transform(user_embeddings))\n",
    "    print(pca.fit_transform(user_embeddings).squeeze())\n",
    "    pca_2_user_embeddings = pca.fit_transform(user_embeddings).squeeze().tolist()\n",
    "    pca_2_user_embeddings = list(map(lambda emb: str(emb), pca_2_user_embeddings))\n",
    "    df.loc[condition, 'pca_2_user_embedding'] = pca_2_user_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we produce the user embedding and movie embedding for every edge in the dataset. These embeddings can then be used for the LLM on the link-prediction task. Can be skipped if this was already done ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_df = gnn_trainer.get_embeddings(movie_lens_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the vanilla encoder only classifier. This classifier does only use the NLP part of the prompt (no graph embeddings) for predicting if the given link exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_encoder_only_classifier = VanillaEncoderOnlyClassifier(movie_lens_loader.llm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a vanilla llm dataset and tokenize it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vanilla = movie_lens_loader.generate_vanilla_dataset(vanilla_encoder_only_classifier.tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model on the produced dataset. This can be skipped, if already trained ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_encoder_only_classifier.train_model_on_data(dataset_vanilla, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the prompt encoder only classifier. This classifier uses the vanilla prompt and the graph embeddings for its link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder_only_classifier = PromptEncoderOnlyClassifier(movie_lens_loader, gnn_trainer.get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate a prompt dataset, this time the prompts also include 2d embeddings of user and movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = movie_lens_loader.generate_prompt_embedding_dataset(prompt_encoder_only_classifier.tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also train the model. This can be skipped if already done ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder_only_classifier.train_model_on_data(dataset_prompt, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_negative_sample = movie_lens_loader.sample_prompt_datapoint(existing=False, get_embedding_cb=gnn_trainer.get_embedding, tokenize_function=prompt_encoder_only_classifier.tokenize_function)\n",
    "prompt_positive_sample = movie_lens_loader.sample_prompt_datapoint(tokenize_function=prompt_encoder_only_classifier.tokenize_function)\n",
    "vanilla_negative_sample = movie_lens_loader.sample_vanilla_datapoint(existing=False, tokenize_function=vanilla_encoder_only_classifier.tokenize_function)\n",
    "vanilla_positive_sample = movie_lens_loader.sample_vanilla_datapoint(tokenize_function=vanilla_encoder_only_classifier.tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", model_max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"user: 0, title: Toy Story (1995), genres: ['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy'],[0.09566975384950638, 0.1871771365404129]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current State\n",
    "Here I want to plot the attentions not only between single tokens but between the embedding part and non-embedding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind\n",
    "\n",
    "def foo(self: PromptEncoderOnlyClassifier, sample: dict, layer = -1):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(input_ids = sample[\"input_ids\"], attention_mask = sample[\"attention_mask\"], output_attentions=True)\n",
    "        attentions = outputs.attentions  # This will contain the attention weights for each layer and head\n",
    "    combined_attention = torch.sum(attentions[layer], dim=1).squeeze().detach().numpy()\n",
    "    # Tokenize the text to get the token labels\n",
    "    tokens = self.tokenizer.convert_ids_to_tokens(sample['input_ids'][0])\n",
    "    starting_index_user_embeddings = find_sub_list(['user', 'em', '##bed', '##ding'], tokens)\n",
    "    starting_index_movie_embeddings = find_sub_list(['user', 'em', '##bed', '##ding'], tokens)\n",
    "    print\n",
    "\n",
    "    # Plot the combined attention weights\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(combined_attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax)\n",
    "    plt.title('Combined Attention Weights for Layer 1 After Linear Projection')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.show()\n",
    "foo(prompt_encoder_only_classifier, prompt_negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder_only_classifier.plot_confusion_matrix(dataset=dataset_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_encoder_only_classifier.plot_confusion_matrix(dataset=dataset_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grundprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
