{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from gnn import GNNTrainer\n",
    "from movie_lens_loader import MovieLensLoader\n",
    "from llm import PromptBertClassifier, VanillaBertClassifier, AddingEmbeddingsBertClassifierBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "KGE_DIMENSION = 2 # Output Dimension of the GNN Encoder.\n",
    "model_max_length = 256 if KGE_DIMENSION <= 8 else 512\n",
    "movie_lens_loader = MovieLensLoader(kge_dimensions = [KGE_DIMENSION])\n",
    "gnn_trainer =    GNNTrainer(movie_lens_loader.data, kge_dimension = KGE_DIMENSION)\n",
    "vanilla_bert_only_classifier = VanillaBertClassifier(movie_lens_loader.llm_df,model_max_length = model_max_length)\n",
    "dataset_vanilla = movie_lens_loader.generate_vanilla_dataset(vanilla_bert_only_classifier.tokenize_function)\n",
    "prompt_bert_only_classifier = PromptBertClassifier(movie_lens_loader, gnn_trainer.get_embedding, kge_dimension=KGE_DIMENSION, batch_size=64,model_max_length = model_max_length)\n",
    "dataset_prompt = movie_lens_loader.generate_prompt_embedding_dataset(prompt_bert_only_classifier.tokenize_function, kge_dimension = prompt_bert_only_classifier.kge_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_trainer_large = GNNTrainer(movie_lens_loader.data, hidden_channels=config.hidden_size)\n",
    "#gnn_trainer_large.train_model(movie_lens_loader.gnn_train_data, 10)\n",
    "#gnn_trainer_large.validate_model(movie_lens_loader.gnn_test_data)\n",
    "gnn_trainer_large.get_embeddings(movie_lens_loader)\n",
    "adding_embedding_bert_only_classifier = AddingEmbeddingsBertClassifierBase(movie_lens_loader, gnn_trainer_large.get_embedding, kge_dimension=config.hidden_size, batch_size=64,model_max_length = model_max_length)\n",
    "dataset_adding_embedding = movie_lens_loader.generate_adding_embedding_dataset(adding_embedding_bert_only_classifier.tokenizer.sep_token, adding_embedding_bert_only_classifier.tokenizer.pad_token, adding_embedding_bert_only_classifier.tokenize_function, kge_dimension = config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_adding_embedding[\"train\"][0][\"graph_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_embedding_bert_only_classifier.train_model_on_data(dataset_adding_embedding, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import ast\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import BertForSequenceClassification, BertModel, BertTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertEmbeddingBertForSequenceClassification(BertForSequenceClassification):\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        graph_embeddings: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.bert.embeddings(input_ids)\n",
    "        if graph_embeddings is not None and len(graph_embeddings) > 0:\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                \n",
    "                mask = ((attention_mask.sum(dim = 1) -1).unsqueeze(1).repeat((1,2))-torch.tensor([3,1])).unsqueeze(2).repeat((1,1,self.config.hidden_size))        \n",
    "                inputs_embeds = inputs_embeds.scatter(1, mask, graph_embeddings)\n",
    "        outputs = self.bert(\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "model = InsertEmbeddingBertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", model_max_length=128)\n",
    "sep_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "cls_token = tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.rand(3, 2, model.config.hidden_size)\n",
    "encodes = tokenizer([f\"test{sep_token}{pad_token}{sep_token}{pad_token}\", f\"test2{sep_token}{pad_token}{sep_token}{pad_token}\"], return_tensors=\"pt\", padding = \"max_length\", truncation=True)\n",
    "embeddings.shape, encodes.input_ids.shape, encodes.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(**encodes, graph_embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (torch.tensor([6, 7]).unsqueeze(1).repeat((1,2))-3).unsqueeze(2).repeat((1,1,3))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.rand(2, 7, 3)\n",
    "print(\"input\", emb.shape)\n",
    "mask_ = torch.tensor([[[2,2,2],[4,4,4]], [[3,3,3],[5,5,5]]])\n",
    "mask = (torch.tensor([6, 7]).unsqueeze(1).repeat((1,2))-torch.tensor([3,1])).unsqueeze(2).repeat((1,1,3))\n",
    "print(\"mask\", mask.shape)\n",
    "values = torch.rand(2, 2, 3)\n",
    "print(\"assign\", values.shape)\n",
    "print(\"mask\", mask)\n",
    "print(\"assign\", values)\n",
    "print(\"input\", emb)\n",
    "print(emb.scatter(1, mask, values))\n",
    "#emb[mask] = torch.tensor([0.111,0.222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vanilla_bert_only_classifier.model\n",
    "tokenize_function = vanilla_bert_only_classifier.tokenize_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = movie_lens_loader.sample_vanilla_datapoint(existing=False)\n",
    "sample = tokenize_function(sample, return_pt=True)\n",
    "output = model.forward(input_ids = sample[\"input_ids\"], attention_mask=sample[\"attention_mask\"])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_function = prompt_bert_only_classifier.tokenize_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sample = movie_lens_loader.sample_prompt_datapoint(gnn_trainer.get_embedding, kgeg_dimension=KGE_DIMENSION)\n",
    "prompt_sample_tokenized = tokenize_function(prompt_sample, return_pt=True)\n",
    "print(prompt_sample)\n",
    "model.forward(input_ids = prompt_sample_tokenized[\"input_ids\"], attention_mask=prompt_sample_tokenized[\"attention_mask\"], extra_features=torch.tensor([-1.1454768180847168, 2.0188798904418945]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_bert_only_classifier.plot_confusion_matrix(dataset=dataset_vanilla, split = \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prompt_bert_only_classifier.plot_confusion_matrix(dataset=dataset_prompt, split = \"val\") for prompt_bert_only_classifier, dataset_prompt in zip(prompt_bert_only_classifiers, datasets_prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_negative_sample = movie_lens_loader.sample_prompt_datapoint(existing=False, get_embedding_cb=gnn_trainer.get_embedding, tokenize_function=prompt_bert_only_classifier.tokenize_function)\n",
    "prompt_positive_sample = movie_lens_loader.sample_prompt_datapoint(tokenize_function=prompt_bert_only_classifier.tokenize_function)\n",
    "vanilla_negative_sample = movie_lens_loader.sample_vanilla_datapoint(existing=False, tokenize_function=vanilla_bert_only_classifier.tokenize_function)\n",
    "vanilla_positive_sample = movie_lens_loader.sample_vanilla_datapoint(tokenize_function=vanilla_bert_only_classifier.tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current State\n",
    "Here I want to plot the attentions not only between single tokens but between the embedding part and non-embedding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind\n",
    "\n",
    "def foo(self: PromptBertClassifier, sample: dict, layer = -1):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(input_ids = sample[\"input_ids\"], attention_mask = sample[\"attention_mask\"], output_attentions=True)\n",
    "        attentions = outputs.attentions  # This will contain the attention weights for each layer and head\n",
    "    combined_attention = torch.sum(attentions[layer], dim=1).squeeze().detach().numpy()\n",
    "    # Tokenize the text to get the token labels\n",
    "    tokens = self.tokenizer.convert_ids_to_tokens(sample['input_ids'][0])\n",
    "    print(tokens)\n",
    "    starting_index_user_embeddings = find_sub_list(['user', 'em', '##bed', '##ding', ':', '['], tokens)\n",
    "    starting_index_movie_embeddings = find_sub_list(['movie', 'em', '##bed', '##ding', ':', '['], tokens)\n",
    "\n",
    "    # Plot the combined attention weights\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(combined_attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax)\n",
    "    plt.title('Combined Attention Weights for Layer 1 After Linear Projection')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.show()\n",
    "foo(prompt_bert_only_classifiers[0], prompt_negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grundprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
