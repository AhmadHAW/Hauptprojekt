{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_manager.movie_lens_manager import (\n",
    "    MovieLensManager,\n",
    "    ROOT,\n",
    ")\n",
    "from llm_manager.vanilla.classifier import VanillaClassifier\n",
    "from llm_manager.vanilla.config import VANILLA_TOKEN_TYPE_VALUES\n",
    "from utils import get_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_manager = MovieLensManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VANILLA_ROOT = f\"{ROOT}/llm/vanilla\"\n",
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VanillaBertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VanillaBertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.token_type_embeddings.weight: found shape torch.Size([2, 128]) in the checkpoint and torch.Size([3, 128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vanilla_bert_classifier = VanillaClassifier(\n",
    "    kg_manager.llm_df,\n",
    "    kg_manager.source_df,\n",
    "    kg_manager.target_df,\n",
    "    root_path=VANILLA_ROOT,\n",
    "    model_name=MODEL_NAME,\n",
    "    false_ratio=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vanilla = kg_manager.generate_vanilla_dataset(\n",
    "    vanilla_bert_classifier.tokenize_function, force_recompute=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_id', 'target_id', 'split', 'labels', 'prompt_feature_title', 'prompt_feature_genres', 'prompt', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 56469\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source_id', 'target_id', 'split', 'labels', 'prompt_feature_title', 'prompt_feature_genres', 'prompt', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 20166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source_id', 'target_id', 'split', 'labels', 'prompt_feature_title', 'prompt_feature_genres', 'prompt', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 20166\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_list = [\"cls\", \"user_id\", \"movie_id\", \"title\", \"genres\", \"seps\"]\n",
    "key_combinations = get_combinations(list(set(VANILLA_TOKEN_TYPE_VALUES)))\n",
    "len(key_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split test\n",
      "forwarding without masking ./data/llm/vanilla/attentions/split_test_pos_0_com_[].npy\n",
      "combination: frozenset()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split val\n",
      "forwarding without masking ./data/llm/vanilla/attentions/split_val_pos_0_com_[].npy\n",
      "combination: frozenset()\n"
     ]
    }
   ],
   "source": [
    "vanilla_bert_classifier.forward_dataset_and_save_outputs(\n",
    "    dataset=dataset_vanilla,\n",
    "    batch_size=1024,\n",
    "    save_step_size=1000,\n",
    "    splits=[\"test\", \"val\"],\n",
    "    combination_boundaries=(0, 1),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
