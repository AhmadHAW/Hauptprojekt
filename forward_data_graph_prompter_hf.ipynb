{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_manager.movie_lens_manager import (\n",
    "    MovieLensManager,\n",
    "    ROOT,\n",
    ")\n",
    "from graph_representation_generator.graph_representation_generator import (\n",
    "    GraphRepresentationGenerator,\n",
    ")\n",
    "from llm_manager.graph_prompter_hf.classifier import GraphPrompterHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_manager = MovieLensManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VANILLA_ROOT = f\"{ROOT}/llm/vanilla\"\n",
    "GRAPH_PROMPTER_HF_ROOT = f\"{ROOT}/llm/graph_prompter_hf\"\n",
    "GRAPH_PROMPTER_HF_FROZEN_ROOT = f\"{ROOT}/llm/graph_prompter_hf_frozen\"\n",
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model\n",
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "graph_representation_generator_graph_prompter_hf = GraphRepresentationGenerator(\n",
    "    kg_manager.data.to(\"cuda\"),\n",
    "    kg_manager.gnn_train_data.to(\"cuda\"),\n",
    "    kg_manager.gnn_val_data.to(\"cuda\"),\n",
    "    kg_manager.gnn_test_data.to(\"cuda\"),\n",
    "    hidden_channels=128,\n",
    "    kge_dimension=128,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "graph_prompter_hf_bert_classifier = GraphPrompterHF(\n",
    "    kg_manager,\n",
    "    graph_representation_generator_graph_prompter_hf.get_embeddings,\n",
    "    MODEL_NAME,\n",
    "    root_path=GRAPH_PROMPTER_HF_ROOT,\n",
    "    false_ratio=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dataset_graph_prompter_hf = kg_manager.shard_dataset_randomly(\n",
    "    VANILLA_ROOT,\n",
    "    GRAPH_PROMPTER_HF_FROZEN_ROOT,\n",
    "    GRAPH_PROMPTER_HF_ROOT,\n",
    "    shard_size=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_manager.graph_prompter_hf.config import (\n",
    "    GRAPH_PROMPTER_TOKEN_TYPE_VALUES,\n",
    "    GRAPH_PROMPTER_TOKEN_TYPE_VALUES_REVERSE,\n",
    ")\n",
    "from utils import get_combinations\n",
    "\n",
    "\n",
    "token_type_values = list(set(GRAPH_PROMPTER_TOKEN_TYPE_VALUES))\n",
    "token_type_values_reverse = GRAPH_PROMPTER_TOKEN_TYPE_VALUES_REVERSE\n",
    "token_type_combinations = get_combinations(token_type_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(token_type_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split test\n",
      "combination: frozenset({0, 1, 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combination: frozenset({0, 1, 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001A15A737530>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\MARS\\.conda\\envs\\hauptprojekt\\Lib\\threading.py\", line 1533, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "graph_prompter_hf_bert_classifier.forward_dataset_and_save_outputs(\n",
    "    dataset=dataset_graph_prompter_hf,\n",
    "    batch_size=512,\n",
    "    save_step_size=1250,\n",
    "    # combination_boundaries=(22, len(token_type_combinations) - 1),\n",
    "    splits=[\"test\", \"val\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
